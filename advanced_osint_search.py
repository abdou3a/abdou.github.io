#!/usr/bin/env python3
"""
OSINT Advanced Search - Recherche approfondie avec techniques r√©elles
Target: ABDELILAH ABDELLAOUI
"""

import requests
import json
import time
from urllib.parse import quote
import re

class RealOSINTSearch:
    """Recherche OSINT avec de vraies techniques"""
    
    def __init__(self):
        self.target = "ABDELILAH ABDELLAOUI"
        self.results = {}
        
    def search_github_users(self):
        """Recherche sur GitHub API"""
        print("üêô Recherche GitHub...")
        try:
            url = f"https://api.github.com/search/users?q={quote(self.target)}"
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                users = data.get('items', [])
                
                github_profiles = []
                for user in users[:5]:  # Top 5 r√©sultats
                    profile = {
                        'username': user.get('login'),
                        'profile_url': user.get('html_url'),
                        'avatar': user.get('avatar_url'),
                        'type': user.get('type'),
                        'public_repos': user.get('public_repos', 0)
                    }
                    github_profiles.append(profile)
                
                self.results['github'] = {
                    'found': len(github_profiles),
                    'profiles': github_profiles
                }
                print(f"‚úÖ {len(github_profiles)} profils GitHub trouv√©s")
                return github_profiles
            else:
                print(f"‚ùå Erreur GitHub API: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"‚ùå Erreur GitHub: {e}")
            return []
    
    def analyze_name_linguistics(self):
        """Analyse linguistique du nom"""
        print("üî§ Analyse linguistique...")
        
        analysis = {
            'first_name': 'ABDELILAH',
            'last_name': 'ABDELLAOUI',
            'origin': {
                'likely_origin': 'Maghreb (Maroc, Alg√©rie, Tunisie)',
                'linguistic_family': 'Arabe/Berb√®re',
                'religious_context': 'Musulman (Abd Allah = Serviteur de Dieu)',
                'gender': 'Masculin'
            },
            'variations': {
                'first_name_variants': [
                    'Abdelilah', 'Abd Al-Ilah', 'Abdel Ilah',
                    'ÿπÿ®ÿØ ÿßŸÑÿ•ŸÑŸá', 'Abdelilahe'
                ],
                'last_name_variants': [
                    'Abdellaoui', 'Abd Ellaoui', 'Abdel Laoui',
                    'ÿπÿ®ÿØ ÿßŸÑÿπÿßŸàŸä', 'Abdelaoui'
                ]
            },
            'social_handles': [
                'abdelilah.abdellaoui', 'abdelilahabdellaoui',
                'a.abdellaoui', 'abdelilah_a', 'abdellaoui.a'
            ]
        }
        
        self.results['linguistic_analysis'] = analysis
        print("‚úÖ Analyse linguistique termin√©e")
        return analysis
    
    def check_domain_variations(self):
        """V√©rifie les variations de domaines"""
        print("üåê V√©rification des domaines...")
        
        base_variations = [
            'abdelilahabdellaoui',
            'a.abdellaoui',
            'abdelilah.abdellaoui',
            'abdellaoui.abdelilah'
        ]
        
        domain_results = []
        for variation in base_variations:
            domains = [
                f"{variation}.com",
                f"{variation}.org",
                f"{variation}.net",
                f"{variation}.me"
            ]
            
            for domain in domains:
                try:
                    # Test DNS simple (pas de ping pour √©viter les blocages)
                    import socket
                    socket.gethostbyname(domain)
                    domain_results.append({
                        'domain': domain,
                        'status': 'exists',
                        'variation': variation
                    })
                except socket.gaierror:
                    domain_results.append({
                        'domain': domain,
                        'status': 'available',
                        'variation': variation
                    })
                except Exception as e:
                    domain_results.append({
                        'domain': domain,
                        'status': f'error: {e}',
                        'variation': variation
                    })
        
        self.results['domains'] = domain_results
        existing_domains = [d for d in domain_results if d['status'] == 'exists']
        print(f"‚úÖ {len(existing_domains)} domaines existants trouv√©s sur {len(domain_results)} test√©s")
        return domain_results
    
    def search_professional_keywords(self):
        """Recherche avec mots-cl√©s professionnels"""
        print("üíº G√©n√©ration de mots-cl√©s professionnels...")
        
        # Bas√© sur l'analyse du nom et du contexte
        professional_keywords = {
            'tech_fields': [
                'd√©veloppeur', 'ing√©nieur', 'informatique', 'software',
                'web developer', 'programmer', 'data scientist'
            ],
            'business_fields': [
                'entrepreneur', 'manager', 'consultant', 'directeur',
                'CEO', 'founder', 'business analyst'
            ],
            'academic_fields': [
                'professeur', 'chercheur', 'PhD', 'docteur',
                'universit√©', 'recherche', 'acad√©mique'
            ],
            'geographic_keywords': [
                'Maroc', 'Morocco', 'Casablanca', 'Rabat',
                'Alg√©rie', 'Algeria', 'Tunisie', 'Tunisia',
                'France', 'Canada', 'Belgium'
            ]
        }
        
        # G√©n√©ration de requ√™tes combin√©es
        combined_queries = []
        name = self.target
        
        for category, keywords in professional_keywords.items():
            for keyword in keywords[:3]:  # Top 3 par cat√©gorie
                combined_queries.append(f'"{name}" {keyword}')
                combined_queries.append(f'"{name}" "{keyword}"')
        
        self.results['professional_search'] = {
            'keywords': professional_keywords,
            'generated_queries': combined_queries[:20]  # Top 20
        }
        
        print(f"‚úÖ {len(combined_queries)} requ√™tes professionnelles g√©n√©r√©es")
        return professional_keywords
    
    def check_social_media_apis(self):
        """V√©rification sur les APIs publiques des r√©seaux sociaux"""
        print("üì± V√©rification APIs r√©seaux sociaux...")
        
        # Note: La plupart des APIs sociales n√©cessitent des cl√©s
        # Voici les endpoints publics disponibles
        
        social_apis = {
            'github': {
                'endpoint': f"https://api.github.com/search/users?q={quote(self.target)}",
                'status': 'available',
                'auth_required': False
            },
            'reddit': {
                'endpoint': f"https://www.reddit.com/search.json?q={quote(self.target)}",
                'status': 'available',
                'auth_required': False
            },
            'twitter': {
                'endpoint': 'https://api.twitter.com/2/users/by/username/',
                'status': 'requires_auth',
                'auth_required': True
            },
            'linkedin': {
                'endpoint': 'https://api.linkedin.com/v2/people',
                'status': 'requires_auth',
                'auth_required': True
            }
        }
        
        # Test Reddit API
        try:
            reddit_url = f"https://www.reddit.com/search.json?q={quote(self.target)}"
            response = requests.get(reddit_url, 
                                  headers={'User-Agent': 'OSINT-Research-Bot/1.0'},
                                  timeout=10)
            
            if response.status_code == 200:
                reddit_data = response.json()
                posts = reddit_data.get('data', {}).get('children', [])
                
                social_apis['reddit']['results'] = len(posts)
                social_apis['reddit']['posts'] = [
                    {
                        'title': post['data'].get('title', ''),
                        'subreddit': post['data'].get('subreddit', ''),
                        'url': f"https://reddit.com{post['data'].get('permalink', '')}"
                    }
                    for post in posts[:5]  # Top 5
                ]
            
        except Exception as e:
            social_apis['reddit']['error'] = str(e)
        
        self.results['social_apis'] = social_apis
        print("‚úÖ V√©rification APIs termin√©e")
        return social_apis
    
    def generate_osint_report(self):
        """G√©n√®re le rapport OSINT final"""
        print("\n" + "="*70)
        print("üéØ RAPPORT OSINT AVANC√â - ABDELILAH ABDELLAOUI")
        print("="*70)
        
        print(f"\nüìä R√âSUM√â EX√âCUTIF:")
        print(f"‚Ä¢ Cible: {self.target}")
        print(f"‚Ä¢ Date: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"‚Ä¢ Techniques: Google Dorking, APIs publiques, Analyse linguistique")
        
        # GitHub
        if 'github' in self.results:
            github = self.results['github']
            print(f"\nüêô GITHUB:")
            print(f"‚Ä¢ Profils trouv√©s: {github['found']}")
            for profile in github['profiles']:
                print(f"  - {profile['username']}: {profile['profile_url']}")
                print(f"    Repos publics: {profile['public_repos']}")
        
        # Analyse linguistique
        if 'linguistic_analysis' in self.results:
            ling = self.results['linguistic_analysis']
            print(f"\nüî§ ANALYSE LINGUISTIQUE:")
            print(f"‚Ä¢ Origine probable: {ling['origin']['likely_origin']}")
            print(f"‚Ä¢ Genre: {ling['origin']['gender']}")
            print(f"‚Ä¢ Contexte religieux: {ling['origin']['religious_context']}")
            
            print(f"‚Ä¢ Variations du pr√©nom:")
            for var in ling['variations']['first_name_variants']:
                print(f"  - {var}")
            
            print(f"‚Ä¢ Handles sociaux sugg√©r√©s:")
            for handle in ling['social_handles']:
                print(f"  - {handle}")
        
        # Domaines
        if 'domains' in self.results:
            existing_domains = [d for d in self.results['domains'] if d['status'] == 'exists']
            print(f"\nüåê DOMAINES:")
            print(f"‚Ä¢ Domaines existants: {len(existing_domains)}")
            for domain in existing_domains:
                print(f"  - {domain['domain']} (bas√© sur: {domain['variation']})")
        
        # Recherche professionnelle
        if 'professional_search' in self.results:
            prof = self.results['professional_search']
            print(f"\nüíº RECHERCHE PROFESSIONNELLE:")
            print(f"‚Ä¢ Requ√™tes g√©n√©r√©es: {len(prof['generated_queries'])}")
            print("‚Ä¢ Top 5 requ√™tes recommand√©es:")
            for i, query in enumerate(prof['generated_queries'][:5], 1):
                print(f"  {i}. {query}")
        
        # APIs sociales
        if 'social_apis' in self.results:
            apis = self.results['social_apis']
            print(f"\nüì± APIS R√âSEAUX SOCIAUX:")
            
            if 'reddit' in apis and 'results' in apis['reddit']:
                print(f"‚Ä¢ Reddit: {apis['reddit']['results']} mentions trouv√©es")
                if 'posts' in apis['reddit']:
                    for post in apis['reddit']['posts'][:3]:
                        print(f"  - r/{post['subreddit']}: {post['title'][:50]}...")
        
        print(f"\n‚ö†Ô∏è RECOMMANDATIONS:")
        print("‚Ä¢ Utiliser les requ√™tes Google Dorking list√©es")
        print("‚Ä¢ V√©rifier manuellement les profils GitHub trouv√©s")
        print("‚Ä¢ Rechercher sur LinkedIn avec les variations du nom")
        print("‚Ä¢ Contr√¥ler les domaines existants pour des sites personnels")
        print("‚Ä¢ Utiliser les handles sugg√©r√©s sur les r√©seaux sociaux")
        
        print(f"\n‚öñÔ∏è L√âGAL & √âTHIQUE:")
        print("‚Ä¢ Toutes les donn√©es proviennent de sources publiques")
        print("‚Ä¢ Respecter la vie priv√©e et les conditions d'utilisation")
        print("‚Ä¢ Usage exclusivement l√©gal et √©thique")
        
        print("\n" + "="*70)
    
    def run_complete_search(self):
        """Lance la recherche compl√®te"""
        print("üöÄ OSINT-AI Platform - Recherche Avanc√©e")
        print(f"üéØ Target: {self.target}")
        print("="*50)
        
        self.analyze_name_linguistics()
        self.search_github_users()
        self.check_domain_variations()
        self.search_professional_keywords()
        self.check_social_media_apis()
        
        # G√©n√©ration du rapport
        self.generate_osint_report()
        
        # Sauvegarde
        output_file = f"/workspaces/abdou.github.io/advanced_osint_report_{int(time.time())}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ Rapport d√©taill√© sauvegard√©: {output_file}")
        return self.results

if __name__ == "__main__":
    searcher = RealOSINTSearch()
    searcher.run_complete_search()
